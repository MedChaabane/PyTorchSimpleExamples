{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![](img/the_real_reason.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "# PyTorch Overview\n",
    "\n",
    "PyTorch uses an imperative / eager paradigm. That is, each line of code required to build a graph defines a component of that graph. We can independently perform computations on these components itself, even before your graph is built completely. This is called “define-by-run” methodology.\n",
    "\n",
    "![](img/dynamic_graph.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a much better explanation of PyTorch (I think)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[1;32m<ipython-input-1-72089dcf9312>\u001b[0m(7)\u001b[0;36mf\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m      5 \u001b[1;33m    \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m      6 \u001b[1;33m    \u001b[0mset_trace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# <-- OMG! =D\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m----> 7 \u001b[1;33m    \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m      8 \u001b[1;33m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m      9 \u001b[1;33m\u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> x\n",
      "tensor([[ 0.9527,  1.2284,  0.2374,  0.3110,  0.3224,  1.6226, -0.1946,  0.8746,\n",
      "          0.4749, -0.2934]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "def f(x):\n",
    "    res = x + x\n",
    "    set_trace()  # <-- OMG! =D\n",
    "    return res\n",
    "\n",
    "x = torch.randn(1, 10)\n",
    "f(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I like pytorch because\n",
    "- \"it's just stupid python\"\n",
    "- easy to debug\n",
    "- nice and extensible interface\n",
    "- research-y feel\n",
    "- research is often published as pytorch project\n",
    "\n",
    "\n",
    "But also Tensorflow has its advantages:\n",
    "-  fast in some cases (multiple machines)\n",
    "-  deployment\n",
    "-  Visualization\n",
    "-  documentation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init, helpers, utils, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "from IPython.core.debugger import set_trace\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensors\n",
    "Tensors - the atoms of machine learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch as NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "from numpy.linalg import multi_dot as mdot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numpy\n",
    "np.eye(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch\n",
    "torch.eye(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.2466819 , 0.632842  , 0.89119311],\n",
       "       [0.77079608, 0.10957379, 0.01142741],\n",
       "       [0.0501221 , 0.73080843, 0.75285245],\n",
       "       [0.42087822, 0.40558455, 0.21842931],\n",
       "       [0.61992209, 0.75119631, 0.55434517]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numpy\n",
    "X = np.random.random((5, 3))\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2083, 0.8134, 0.6836],\n",
       "        [0.1490, 0.5247, 0.1851],\n",
       "        [0.9102, 0.8258, 0.4530],\n",
       "        [0.5562, 0.6065, 0.6818],\n",
       "        [0.9099, 0.0940, 0.2565]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pytorch\n",
    "Y = torch.rand((5, 3))\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numpy\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.21893267, 0.91358426, 0.70196692],\n",
       "       [0.91358426, 1.6753711 , 1.6204411 ],\n",
       "       [0.70196692, 1.6204411 , 1.71615249]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numpy\n",
    "X.T @ X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.0313, 1.4222, 1.1950],\n",
       "        [1.4222, 1.9955, 1.4649],\n",
       "        [1.1950, 1.4649, 1.2374]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch\n",
    "Y.t() @ Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2.00389971,  -3.45835662,   2.44581522],\n",
       "       [ -3.45835662,  12.85062471, -10.71934372],\n",
       "       [  2.44581522, -10.71934372,   9.70379018]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numpy\n",
    "inv(X.T @ X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1400, -0.0330, -1.0618],\n",
       "        [-0.0330,  3.8270, -4.4985],\n",
       "        [-1.0618, -4.4985,  7.1589]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch\n",
    "torch.inverse(Y.t() @ Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More on PyTorch Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Operations are also available as methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 1., 1.],\n",
       "        [1., 2., 1.],\n",
       "        [1., 1., 2.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.eye(3)\n",
    "A.add(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 1., 1.],\n",
       "        [1., 2., 1.],\n",
       "        [1., 1., 2.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.add_(1)\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing and broadcasting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 1., 1.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 1., 1.],\n",
       "        [1., 2., 1.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [2., 1.],\n",
       "        [1., 2.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A[:, 1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[2., 1., 1.],\n",
       "        [1., 2., 1.],\n",
       "        [1., 1., 2.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(A.shape)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.5545, 0.5392, 0.3096]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B= torch.rand((1, 3))\n",
    "print(B.shape)\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.5545, 1.5392, 1.3096],\n",
       "        [1.5545, 2.5392, 1.3096],\n",
       "        [1.5545, 1.5392, 2.3096]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A+B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.eye(3)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch --> numpy\n",
    "A.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numpy --> torch\n",
    "torch.from_numpy(np.eye(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch import autograd  # you rarely use it directly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Suppose, your model is described like this:\n",
    "\n",
    "b = w1 * a\n",
    "\n",
    "c = w2 * a \n",
    "\n",
    "d = (w3 * b) + (w4 * c)\n",
    "\n",
    "L = f(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/image1.png\" width=\"450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/image2.png\" width=\"450\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.])\n",
      "None\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "b = torch.ones(1)\n",
    "print(b.data)\n",
    "print(b.grad)\n",
    "print(b.requires_grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/image3.png\" width=\"450\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.ones(1, requires_grad=True)\n",
    "w.requires_grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.ones(1) * 2\n",
    "print(z)\n",
    "z.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.], grad_fn=<ThAddBackward>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = w + z\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total.backward()\n",
    "w.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total.backward()\n",
    "w.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total.backward()\n",
    "w.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    total = w + z\n",
    "\n",
    "total.requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# But what about the GPU?\n",
    "How do I use the GPU?\n",
    "\n",
    "If you have a GPU make sure that the right pytorch is installed\n",
    "\n",
    "```\n",
    "conda install pytorch torchvision cuda91 -c pytorch\n",
    "```\n",
    "Check https://pytorch.org/ for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a GPU you should get something like: \n",
    "`device(type='cuda', index=0)`\n",
    "\n",
    "You can move data to the GPU by doing `.to(device)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.eye(3)\n",
    "data.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Simple sequential model\n",
    "layers = OrderedDict([\n",
    "    ('conv1', nn.Conv2d(in_channels=1, out_channels=20, kernel_size=5)),\n",
    "    ('relu1', nn.ReLU()),\n",
    "    ('conv2', nn.Conv2d(20,64,5)),\n",
    "    ('relu2', nn.ReLU())\n",
    "])\n",
    "model = nn.Sequential(layers)\n",
    "\n",
    "# output = model(some_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also if we want to build more complex models, we may subclass provided nn.Module class. And of course, these two approaches can be mixed with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(in_features=16 * 5 * 5, out_features=120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LinReg with PyTorch, Gradient Descent, and GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1d1e197afd0>]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEr6ISEMo4YuINMT/AyLIf6xPy8GmAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d1e198d0f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "n_features = 1\n",
    "n_samples = 100\n",
    "\n",
    "X, y = make_regression(\n",
    "    n_samples=n_samples,\n",
    "    n_features=n_features,\n",
    "    noise=10,\n",
    ")\n",
    "print(y.shape)\n",
    "fix, ax = plt.subplots()\n",
    "ax.plot(X, y, \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = torch.from_numpy(X).float()\n",
    "y = torch.from_numpy(y.reshape((n_samples, 1))).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "class LinReg(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.beta = nn.Linear(input_dim, 1)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.beta(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Move everything to GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = LinReg(n_features).to(device)  # <-- here\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "X, y = X.to(device), y.to(device)  # <-- here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w:  Parameter containing:\n",
      "tensor([[-0.5694]], requires_grad=True)\n",
      "b:  Parameter containing:\n",
      "tensor([-0.7658], requires_grad=True)\n",
      "dL/dw:  None\n",
      "dL/db:  None\n"
     ]
    }
   ],
   "source": [
    "# Print out model parameters.\n",
    "print ('w: ', model.beta.weight)\n",
    "print ('b: ', model.beta.bias)\n",
    "\n",
    "# Print out the gradients.\n",
    "print ('dL/dw: ', model.beta.weight.grad) \n",
    "print ('dL/db: ', model.beta.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dL/dw:  tensor([[-147.4653]])\n",
      "dL/db:  tensor([-19.9145])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEICAYAAABcVE8dAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d1e1d41e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train step\n",
    "model.train()\n",
    "optimizer.zero_grad()\n",
    "\n",
    "y_ = model(X)\n",
    "loss = criterion(y_, y)\n",
    "loss.backward(loss)\n",
    "# Print out the gradients.\n",
    "print ('dL/dw: ', model.beta.weight.grad) \n",
    "print ('dL/db: ', model.beta.bias.grad)\n",
    "\n",
    "optimizer.step()\n",
    "# for f in model.parameters():\n",
    "#     f.data.sub_(f.grad.data * learning_rate)\n",
    "\n",
    "\n",
    "# Eval\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_ = model(X)    \n",
    "\n",
    "# Vis\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(X.cpu().numpy(), y_.cpu().numpy(), \".\", label=\"pred\")\n",
    "ax.plot(X.cpu().numpy(), y.cpu().numpy(), \".\", label=\"data\")\n",
    "ax.set_title(f\"MSE: {loss.item():0.1f}\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging\n",
    "\n",
    "\n",
    "## IPDB cheatsheet\n",
    "IPython Debugger\n",
    "\n",
    "Taken from http://frid.github.io/blog/2014/06/05/python-ipdb-cheatsheet/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debuging a NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4554, 0.0877, 0.2672],\n",
       "        [0.5362, 0.0036, 0.5139],\n",
       "        [0.1420, 0.9670, 0.6439],\n",
       "        [0.7929, 0.9761, 0.7763],\n",
       "        [0.8374, 0.5822, 0.3811]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.debugger import set_trace\n",
    "X = torch.rand((5, 3))\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[1;32m<ipython-input-49-36ed13c8db25>\u001b[0m(8)\u001b[0;36mforward\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m      6 \u001b[1;33m    \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m      7 \u001b[1;33m        \u001b[0mset_trace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m----> 8 \u001b[1;33m        \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m      9 \u001b[1;33m        \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     10 \u001b[1;33m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> \n",
      "ipdb> \n",
      "ipdb> exit\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-36ed13c8db25>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMyModule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0my_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32massert\u001b[0m \u001b[0my_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ameni\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 477\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    478\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-49-36ed13c8db25>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mset_trace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-49-36ed13c8db25>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mset_trace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ameni\\Anaconda3\\lib\\bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[1;34m(self, frame, event, arg)\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;31m# None\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'line'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'call'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ameni\\Anaconda3\\lib\\bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[1;34m(self, frame)\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class MyModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(3, 1)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        set_trace()\n",
    "        x = self.lin(X)\n",
    "        return X\n",
    "\n",
    "    \n",
    "model = MyModule()\n",
    "y_ = model(X)\n",
    "\n",
    "assert y_.shape == (5, 1), y_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset / Dataset Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Dataset class: It's a nice and simple abstraction to work with data.\n",
    "\n",
    "DataLoader class offers batch loading of datasets with multi-processing and different sample strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class MyDataSet(Dataset):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return 0\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape    (759, 9)\n",
      "x_data shape   torch.Size([759, 8])\n",
      "y_data shape   torch.Size([759, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "xy = np.loadtxt('./data/diabetes.csv.gz', delimiter=',', dtype=np.float32)\n",
    "print('data shape   ',xy.shape)\n",
    "x_data = torch.from_numpy(xy[:, 0:-1])\n",
    "y_data = torch.from_numpy(xy[:, [-1]])\n",
    "\n",
    "print('x_data shape  ',x_data.data.shape)\n",
    "print('y_data shape  ',y_data.data.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: torch.Size([100, 8])\n",
      "y: torch.Size([100, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class DiabetesDataset(Dataset):\n",
    "    \"\"\" Diabetes dataset.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        xy = np.loadtxt('./data/diabetes.csv',\n",
    "                        delimiter=',', dtype=np.float32)\n",
    "        self.len = xy.shape[0]\n",
    "        self.x_data = torch.from_numpy(xy[:, 0:-1])\n",
    "        self.y_data = torch.from_numpy(xy[:, [-1]])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "\n",
    "dataset = DiabetesDataset()\n",
    "train_loader = DataLoader(dataset=dataset,\n",
    "                          batch_size=100,\n",
    "                          shuffle=True)\n",
    "                          \n",
    "\n",
    "X, y = next(iter(train_loader))\n",
    "print(\"X:\", X.shape)\n",
    "print(\"y:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ameni\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='elementwise_mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0.6587796211242676\n",
      "0 1 0.6636003255844116\n",
      "0 2 0.6683725118637085\n",
      "0 3 0.6069745421409607\n",
      "0 4 0.6335617303848267\n",
      "0 5 0.6280879378318787\n",
      "0 6 0.6635976433753967\n",
      "0 7 0.659662127494812\n",
      "1 0 0.6481443047523499\n",
      "1 1 0.6324760317802429\n",
      "1 2 0.6263129115104675\n",
      "1 3 0.6147372722625732\n",
      "1 4 0.6413322687149048\n",
      "1 5 0.6471760272979736\n",
      "1 6 0.6811887621879578\n",
      "1 7 0.6979438662528992\n",
      "2 0 0.6476355195045471\n",
      "2 1 0.6368202567100525\n",
      "2 2 0.6473640203475952\n",
      "2 3 0.6638011932373047\n",
      "2 4 0.6472982168197632\n",
      "2 5 0.6974446177482605\n",
      "2 6 0.6311383247375488\n",
      "2 7 0.5569713115692139\n",
      "3 0 0.5824112892150879\n",
      "3 1 0.6771924495697021\n",
      "3 2 0.7249203324317932\n",
      "3 3 0.6412944793701172\n",
      "3 4 0.6411029696464539\n",
      "3 5 0.6296011805534363\n",
      "3 6 0.5821208357810974\n",
      "3 7 0.7126212120056152\n",
      "4 0 0.6173632144927979\n",
      "4 1 0.6408743262290955\n",
      "4 2 0.6467642784118652\n",
      "4 3 0.6527724266052246\n",
      "4 4 0.6287238001823425\n",
      "4 5 0.6224675178527832\n",
      "4 6 0.6713994741439819\n",
      "4 7 0.7025353312492371\n",
      "5 0 0.6112071871757507\n",
      "5 1 0.6466808319091797\n",
      "5 2 0.6711805462837219\n",
      "5 3 0.6468533277511597\n",
      "5 4 0.6708536744117737\n",
      "5 5 0.6767849922180176\n",
      "5 6 0.5885215997695923\n",
      "5 7 0.6500272154808044\n",
      "6 0 0.6102907061576843\n",
      "6 1 0.6466754078865051\n",
      "6 2 0.6468980312347412\n",
      "6 3 0.6530535221099854\n",
      "6 4 0.6343368887901306\n",
      "6 5 0.6842545866966248\n",
      "6 6 0.6469239592552185\n",
      "6 7 0.6296560764312744\n",
      "7 0 0.6836997270584106\n",
      "7 1 0.5800239443778992\n",
      "7 2 0.6720297932624817\n",
      "7 3 0.6468647718429565\n",
      "7 4 0.6405837535858154\n",
      "7 5 0.6593372821807861\n",
      "7 6 0.6221635937690735\n",
      "7 7 0.6610361337661743\n",
      "8 0 0.6778753399848938\n",
      "8 1 0.6347448825836182\n",
      "8 2 0.6343494653701782\n",
      "8 3 0.6654874682426453\n",
      "8 4 0.6161872744560242\n",
      "8 5 0.6467307209968567\n",
      "8 6 0.6530661582946777\n",
      "8 7 0.618491530418396\n",
      "9 0 0.6466798186302185\n",
      "9 1 0.6468774676322937\n",
      "9 2 0.6152907609939575\n",
      "9 3 0.6533769369125366\n",
      "9 4 0.6592853665351868\n",
      "9 5 0.5838025212287903\n",
      "9 6 0.6792975068092346\n",
      "9 7 0.6941056847572327\n",
      "10 0 0.7342453598976135\n",
      "10 1 0.6051703095436096\n",
      "10 2 0.6466358304023743\n",
      "10 3 0.6161981225013733\n",
      "10 4 0.6652966141700745\n",
      "10 5 0.6037697196006775\n",
      "10 6 0.6656820178031921\n",
      "10 7 0.6081069707870483\n",
      "11 0 0.6660245656967163\n",
      "11 1 0.6154978275299072\n",
      "11 2 0.653195321559906\n",
      "11 3 0.659366250038147\n",
      "11 4 0.6592649817466736\n",
      "11 5 0.6217151880264282\n",
      "11 6 0.6214205026626587\n",
      "11 7 0.6722826957702637\n",
      "12 0 0.6533231139183044\n",
      "12 1 0.6592966318130493\n",
      "12 2 0.640533983707428\n",
      "12 3 0.6404771208763123\n",
      "12 4 0.6655816435813904\n",
      "12 5 0.6342477798461914\n",
      "12 6 0.5905023813247681\n",
      "12 7 0.694040060043335\n",
      "13 0 0.6152151226997375\n",
      "13 1 0.6403968334197998\n",
      "13 2 0.6593387126922607\n",
      "13 3 0.6342278718948364\n",
      "13 4 0.666074275970459\n",
      "13 5 0.6342132091522217\n",
      "13 6 0.6532109975814819\n",
      "13 7 0.6611769795417786\n",
      "14 0 0.6843373775482178\n",
      "14 1 0.6774371862411499\n",
      "14 2 0.6347895264625549\n",
      "14 3 0.6835259199142456\n",
      "14 4 0.6228349208831787\n",
      "14 5 0.6223580241203308\n",
      "14 6 0.566622793674469\n",
      "14 7 0.6831908226013184\n",
      "15 0 0.6909106373786926\n",
      "15 1 0.6712997555732727\n",
      "15 2 0.6223024129867554\n",
      "15 3 0.6651308536529541\n",
      "15 4 0.6284807324409485\n",
      "15 5 0.5911071300506592\n",
      "15 6 0.627841055393219\n",
      "15 7 0.6726066470146179\n",
      "16 0 0.6595921516418457\n",
      "16 1 0.6028595566749573\n",
      "16 2 0.6339890360832214\n",
      "16 3 0.6531668305397034\n",
      "16 4 0.6275891065597534\n",
      "16 5 0.6657968163490295\n",
      "16 6 0.6981447339057922\n",
      "16 7 0.5975653529167175\n",
      "17 0 0.6084048748016357\n",
      "17 1 0.704948365688324\n",
      "17 2 0.6529613733291626\n",
      "17 3 0.602992594242096\n",
      "17 4 0.6080472469329834\n",
      "17 5 0.6600676774978638\n",
      "17 6 0.6536335945129395\n",
      "17 7 0.6832637190818787\n",
      "18 0 0.6784673929214478\n",
      "18 1 0.6280989646911621\n",
      "18 2 0.6467023491859436\n",
      "18 3 0.7158535718917847\n",
      "18 4 0.6040738821029663\n",
      "18 5 0.6281116008758545\n",
      "18 6 0.6029671430587769\n",
      "18 7 0.6614443063735962\n",
      "19 0 0.6531985402107239\n",
      "19 1 0.6278524994850159\n",
      "19 2 0.6277468204498291\n",
      "19 3 0.6080955266952515\n",
      "19 4 0.6925370693206787\n",
      "19 5 0.6081535816192627\n",
      "19 6 0.6599746942520142\n",
      "19 7 0.7051877379417419\n",
      "20 0 0.6716728806495667\n",
      "20 1 0.6776803731918335\n",
      "20 2 0.6769159436225891\n",
      "20 3 0.6108335852622986\n",
      "20 4 0.6222514510154724\n",
      "20 5 0.6655978560447693\n",
      "20 6 0.591655969619751\n",
      "20 7 0.6393707394599915\n",
      "21 0 0.6086363196372986\n",
      "21 1 0.6343536972999573\n",
      "21 2 0.6597657203674316\n",
      "21 3 0.6468945145606995\n",
      "21 4 0.6915605068206787\n",
      "21 5 0.6404186487197876\n",
      "21 6 0.6403152346611023\n",
      "21 7 0.628616452217102\n",
      "22 0 0.6276899576187134\n",
      "22 1 0.6531205773353577\n",
      "22 2 0.6400474309921265\n",
      "22 3 0.6788218021392822\n",
      "22 4 0.6151813268661499\n",
      "22 5 0.6403884887695312\n",
      "22 6 0.6595839858055115\n",
      "22 7 0.6401236653327942\n",
      "23 0 0.6088278293609619\n",
      "23 1 0.6144722700119019\n",
      "23 2 0.6792921423912048\n",
      "23 3 0.6207894086837769\n",
      "23 4 0.6597883105278015\n",
      "23 5 0.6404621601104736\n",
      "23 6 0.7309356927871704\n",
      "23 7 0.576718807220459\n",
      "24 0 0.6789504289627075\n",
      "24 1 0.6153696775436401\n",
      "24 2 0.6401223540306091\n",
      "24 3 0.6336432099342346\n",
      "24 4 0.6596210598945618\n",
      "24 5 0.6402333974838257\n",
      "24 6 0.6278958320617676\n",
      "24 7 0.6722941994667053\n",
      "25 0 0.6718835234642029\n",
      "25 1 0.6279503703117371\n",
      "25 2 0.6278483271598816\n",
      "25 3 0.6722445487976074\n",
      "25 4 0.6465511322021484\n",
      "25 5 0.6531209349632263\n",
      "25 6 0.6530556678771973\n",
      "25 7 0.5762158036231995\n",
      "26 0 0.6661252379417419\n",
      "26 1 0.5827193856239319\n",
      "26 2 0.6932244300842285\n",
      "26 3 0.6465688347816467\n",
      "26 4 0.6145109534263611\n",
      "26 5 0.6209343671798706\n",
      "26 6 0.6334120631217957\n",
      "26 7 0.740362286567688\n",
      "27 0 0.6466427445411682\n",
      "27 1 0.6532021164894104\n",
      "27 2 0.6528658270835876\n",
      "27 3 0.6403084397315979\n",
      "27 4 0.6531769633293152\n",
      "27 5 0.6527290940284729\n",
      "27 6 0.6341879367828369\n",
      "27 7 0.6079704165458679\n",
      "28 0 0.6276317834854126\n",
      "28 1 0.6529320478439331\n",
      "28 2 0.6595772504806519\n",
      "28 3 0.6597913503646851\n",
      "28 4 0.6215611100196838\n",
      "28 5 0.665762722492218\n",
      "28 6 0.627422571182251\n",
      "28 7 0.6396347284317017\n",
      "29 0 0.6338238716125488\n",
      "29 1 0.659802258014679\n",
      "29 2 0.6596090793609619\n",
      "29 3 0.6214138269424438\n",
      "29 4 0.6464059352874756\n",
      "29 5 0.6723791360855103\n",
      "29 6 0.6403135061264038\n",
      "29 7 0.607340931892395\n",
      "30 0 0.6336458325386047\n",
      "30 1 0.6660504937171936\n",
      "30 2 0.6529034972190857\n",
      "30 3 0.6277042627334595\n",
      "30 4 0.6592721343040466\n",
      "30 5 0.6342517733573914\n",
      "30 6 0.6021643877029419\n",
      "30 7 0.7059513926506042\n",
      "31 0 0.6214784979820251\n",
      "31 1 0.6658046841621399\n",
      "31 2 0.6655616760253906\n",
      "31 3 0.6717842817306519\n",
      "31 4 0.5847925543785095\n",
      "31 5 0.6082770824432373\n",
      "31 6 0.6728044152259827\n",
      "31 7 0.6829372048377991\n",
      "32 0 0.6593769192695618\n",
      "32 1 0.6778014898300171\n",
      "32 2 0.6404001712799072\n",
      "32 3 0.5910511612892151\n",
      "32 4 0.6401769518852234\n",
      "32 5 0.665798008441925\n",
      "32 6 0.646618664264679\n",
      "32 7 0.6293690800666809\n",
      "33 0 0.5768836736679077\n",
      "33 1 0.6467244625091553\n",
      "33 2 0.6468865871429443\n",
      "33 3 0.6530221700668335\n",
      "33 4 0.6725262403488159\n",
      "33 5 0.6656978130340576\n",
      "33 6 0.6406031250953674\n",
      "33 7 0.661460280418396\n",
      "34 0 0.646685779094696\n",
      "34 1 0.6023613214492798\n",
      "34 2 0.7104434967041016\n",
      "34 3 0.6095442175865173\n",
      "34 4 0.634004533290863\n",
      "34 5 0.6338930726051331\n",
      "34 6 0.6661891937255859\n",
      "34 7 0.6612218022346497\n",
      "35 0 0.6593056321144104\n",
      "35 1 0.6715297102928162\n",
      "35 2 0.6221910715103149\n",
      "35 3 0.6466843485832214\n",
      "35 4 0.6215589642524719\n",
      "35 5 0.6212135553359985\n",
      "35 6 0.6723387837409973\n",
      "35 7 0.6393160820007324\n",
      "36 0 0.6596355438232422\n",
      "36 1 0.6148682832717896\n",
      "36 2 0.6273968815803528\n",
      "36 3 0.6402185559272766\n",
      "36 4 0.6659612059593201\n",
      "36 5 0.6916185617446899\n",
      "36 6 0.6214109659194946\n",
      "36 7 0.6290633082389832\n",
      "37 0 0.6530125141143799\n",
      "37 1 0.6466034650802612\n",
      "37 2 0.627610445022583\n",
      "37 3 0.665927529335022\n",
      "37 4 0.6589207649230957\n",
      "37 5 0.6718254089355469\n",
      "37 6 0.5908270478248596\n",
      "37 7 0.6398237347602844\n",
      "38 0 0.6531336903572083\n",
      "38 1 0.6850311160087585\n",
      "38 2 0.6528218984603882\n",
      "38 3 0.6029834747314453\n",
      "38 4 0.6527705192565918\n",
      "38 5 0.6338108777999878\n",
      "38 6 0.6402540802955627\n",
      "38 7 0.6289787292480469\n",
      "39 0 0.6146658062934875\n",
      "39 1 0.6724562048912048\n",
      "39 2 0.6272057890892029\n",
      "39 3 0.6403047442436218\n",
      "39 4 0.6271177530288696\n",
      "39 5 0.672823429107666\n",
      "39 6 0.6531971096992493\n",
      "39 7 0.6507226824760437\n",
      "40 0 0.6083001494407654\n",
      "40 1 0.6337682604789734\n",
      "40 2 0.6012139320373535\n",
      "40 3 0.6135016083717346\n",
      "40 4 0.6737114787101746\n",
      "40 5 0.6862853169441223\n",
      "40 6 0.6987016201019287\n",
      "40 7 0.6395304203033447\n",
      "41 0 0.6216438412666321\n",
      "41 1 0.6911723613739014\n",
      "41 2 0.6029238700866699\n",
      "41 3 0.633710503578186\n",
      "41 4 0.6404805183410645\n",
      "41 5 0.7043075561523438\n",
      "41 6 0.6528786420822144\n",
      "41 7 0.5864368081092834\n",
      "42 0 0.6598818302154541\n",
      "42 1 0.6844630241394043\n",
      "42 2 0.6592492461204529\n",
      "42 3 0.6092714071273804\n",
      "42 4 0.6465348601341248\n",
      "42 5 0.6088703274726868\n",
      "42 6 0.6206799149513245\n",
      "42 7 0.6832907795906067\n",
      "43 0 0.6406521797180176\n",
      "43 1 0.6401713490486145\n",
      "43 2 0.6403374671936035\n",
      "43 3 0.6785314679145813\n",
      "43 4 0.6716216802597046\n",
      "43 5 0.615280270576477\n",
      "43 6 0.6402896046638489\n",
      "43 7 0.6179236769676208\n",
      "44 0 0.620800256729126\n",
      "44 1 0.6792572140693665\n",
      "44 2 0.6976504325866699\n",
      "44 3 0.6648668050765991\n",
      "44 4 0.6219179034233093\n",
      "44 5 0.6090520620346069\n",
      "44 6 0.6402499675750732\n",
      "44 7 0.6076601147651672\n",
      "45 0 0.659186601638794\n",
      "45 1 0.6080822348594666\n",
      "45 2 0.6202925443649292\n",
      "45 3 0.6467402577400208\n",
      "45 4 0.6404005289077759\n",
      "45 5 0.6532552242279053\n",
      "45 6 0.6338856220245361\n",
      "45 7 0.728522002696991\n",
      "46 0 0.6589503288269043\n",
      "46 1 0.6589192152023315\n",
      "46 2 0.6150915622711182\n",
      "46 3 0.6591800451278687\n",
      "46 4 0.6216114163398743\n",
      "46 5 0.6656799912452698\n",
      "46 6 0.6217295527458191\n",
      "46 7 0.6613065600395203\n",
      "47 0 0.6653245687484741\n",
      "47 1 0.6275696754455566\n",
      "47 2 0.6594303846359253\n",
      "47 3 0.6402624249458313\n",
      "47 4 0.6337343454360962\n",
      "47 5 0.6340694427490234\n",
      "47 6 0.6784387826919556\n",
      "47 7 0.597337543964386\n",
      "48 0 0.6273660063743591\n",
      "48 1 0.6530531048774719\n",
      "48 2 0.6849852800369263\n",
      "48 3 0.6277235150337219\n",
      "48 4 0.6276850700378418\n",
      "48 5 0.6402186751365662\n",
      "48 6 0.6589708924293518\n",
      "48 7 0.6290261745452881\n",
      "49 0 0.6402671337127686\n",
      "49 1 0.5887496471405029\n",
      "49 2 0.6268457174301147\n",
      "49 3 0.6199312210083008\n",
      "49 4 0.620089590549469\n",
      "49 5 0.6940692067146301\n",
      "49 6 0.6861256957054138\n",
      "49 7 0.7053938508033752\n",
      "50 0 0.6839871406555176\n",
      "50 1 0.6281802654266357\n",
      "50 2 0.5906273722648621\n",
      "50 3 0.6271612048149109\n",
      "50 4 0.6464987993240356\n",
      "50 5 0.6719626784324646\n",
      "50 6 0.6403798460960388\n",
      "50 7 0.6832109093666077\n",
      "51 0 0.634036123752594\n",
      "51 1 0.6466003656387329\n",
      "51 2 0.6342185139656067\n",
      "51 3 0.6654205322265625\n",
      "51 4 0.6027283668518066\n",
      "51 5 0.6468013525009155\n",
      "51 6 0.6718745231628418\n",
      "51 7 0.660126268863678\n",
      "52 0 0.6152976751327515\n",
      "52 1 0.6717793941497803\n",
      "52 2 0.6590720415115356\n",
      "52 3 0.6586447954177856\n",
      "52 4 0.6217260956764221\n",
      "52 5 0.6531472206115723\n",
      "52 6 0.627555251121521\n",
      "52 7 0.6504879593849182\n",
      "53 0 0.6335282921791077\n",
      "53 1 0.6464203000068665\n",
      "53 2 0.6340053081512451\n",
      "53 3 0.6148265600204468\n",
      "53 4 0.6659083366394043\n",
      "53 5 0.6658496856689453\n",
      "53 6 0.6588254570960999\n",
      "53 7 0.6294152736663818\n",
      "54 0 0.6148619651794434\n",
      "54 1 0.6470127701759338\n",
      "54 2 0.6526066064834595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54 3 0.6843398809432983\n",
      "54 4 0.6466639637947083\n",
      "54 5 0.6088714003562927\n",
      "54 6 0.6401572227478027\n",
      "54 7 0.6721425652503967\n",
      "55 0 0.6023809909820557\n",
      "55 1 0.6722416877746582\n",
      "55 2 0.6531356573104858\n",
      "55 3 0.646728515625\n",
      "55 4 0.5958636403083801\n",
      "55 5 0.6463602185249329\n",
      "55 6 0.7044380307197571\n",
      "55 7 0.6287176609039307\n",
      "56 0 0.6273528933525085\n",
      "56 1 0.6463662981987\n",
      "56 2 0.6465644836425781\n",
      "56 3 0.6215323805809021\n",
      "56 4 0.6270335912704468\n",
      "56 5 0.6142587065696716\n",
      "56 6 0.7055595517158508\n",
      "56 7 0.6824511885643005\n",
      "57 0 0.7025354504585266\n",
      "57 1 0.6405131816864014\n",
      "57 2 0.5852620601654053\n",
      "57 3 0.6213696002960205\n",
      "57 4 0.691608726978302\n",
      "57 5 0.6089012622833252\n",
      "57 6 0.6782861351966858\n",
      "57 7 0.6177826523780823\n",
      "58 0 0.6465227007865906\n",
      "58 1 0.6527511477470398\n",
      "58 2 0.7158172726631165\n",
      "58 3 0.6224266886711121\n",
      "58 4 0.6153160929679871\n",
      "58 5 0.614905834197998\n",
      "58 6 0.6400168538093567\n",
      "58 7 0.6504883766174316\n",
      "59 0 0.6275389790534973\n",
      "59 1 0.6463021039962769\n",
      "59 2 0.6718913912773132\n",
      "59 3 0.6462603807449341\n",
      "59 4 0.5959019064903259\n",
      "59 5 0.6658518314361572\n",
      "59 6 0.6467101573944092\n",
      "59 7 0.6613906621932983\n",
      "60 0 0.6402039527893066\n",
      "60 1 0.659005880355835\n",
      "60 2 0.6776460409164429\n",
      "60 3 0.6338188052177429\n",
      "60 4 0.6212124824523926\n",
      "60 5 0.6277819275856018\n",
      "60 6 0.6720008254051208\n",
      "60 7 0.6081741452217102\n",
      "61 0 0.6785991191864014\n",
      "61 1 0.6023569107055664\n",
      "61 2 0.6271510720252991\n",
      "61 3 0.5945497155189514\n",
      "61 4 0.6733713746070862\n",
      "61 5 0.678886890411377\n",
      "61 6 0.6786259412765503\n",
      "61 7 0.607121467590332\n",
      "62 0 0.5953538417816162\n",
      "62 1 0.6858793497085571\n",
      "62 2 0.6522409319877625\n",
      "62 3 0.646696925163269\n",
      "62 4 0.6143100261688232\n",
      "62 5 0.6594915986061096\n",
      "62 6 0.6725764274597168\n",
      "62 7 0.6177377700805664\n",
      "63 0 0.5884515643119812\n",
      "63 1 0.6793417930603027\n",
      "63 2 0.6529402732849121\n",
      "63 3 0.6336917877197266\n",
      "63 4 0.6339965462684631\n",
      "63 5 0.6463060975074768\n",
      "63 6 0.665904700756073\n",
      "63 7 0.6610167622566223\n",
      "64 0 0.6525701880455017\n",
      "64 1 0.6466003656387329\n",
      "64 2 0.6213845014572144\n",
      "64 3 0.6400814652442932\n",
      "64 4 0.6591165661811829\n",
      "64 5 0.6398382782936096\n",
      "64 6 0.6209416389465332\n",
      "64 7 0.6938591003417969\n",
      "65 0 0.5899290442466736\n",
      "65 1 0.6138138771057129\n",
      "65 2 0.6531786918640137\n",
      "65 3 0.6531218886375427\n",
      "65 4 0.6789591312408447\n",
      "65 5 0.6722127795219421\n",
      "65 6 0.6147244572639465\n",
      "65 7 0.7036611437797546\n",
      "66 0 0.646789014339447\n",
      "66 1 0.646683931350708\n",
      "66 2 0.5901806950569153\n",
      "66 3 0.6655257344245911\n",
      "66 4 0.6587962508201599\n",
      "66 5 0.6588066816329956\n",
      "66 6 0.6464943885803223\n",
      "66 7 0.639075756072998\n",
      "67 0 0.6215178966522217\n",
      "67 1 0.6657733917236328\n",
      "67 2 0.6212653517723083\n",
      "67 3 0.677611768245697\n",
      "67 4 0.6402639746665955\n",
      "67 5 0.627641499042511\n",
      "67 6 0.6651754975318909\n",
      "67 7 0.6285555362701416\n",
      "68 0 0.6594306230545044\n",
      "68 1 0.6465633511543274\n",
      "68 2 0.6837055087089539\n",
      "68 3 0.6338294744491577\n",
      "68 4 0.6217619180679321\n",
      "68 5 0.5895677208900452\n",
      "68 6 0.671998918056488\n",
      "68 7 0.6503593921661377\n",
      "69 0 0.6713535189628601\n",
      "69 1 0.6711994409561157\n",
      "69 2 0.6523305773735046\n",
      "69 3 0.5973100066184998\n",
      "69 4 0.6341918706893921\n",
      "69 5 0.6398708820343018\n",
      "69 6 0.6400707960128784\n",
      "69 7 0.6501734256744385\n",
      "70 0 0.6142798662185669\n",
      "70 1 0.6206952929496765\n",
      "70 2 0.6922059059143066\n",
      "70 3 0.6713476777076721\n",
      "70 4 0.6716321706771851\n",
      "70 5 0.6215124726295471\n",
      "70 6 0.6464824676513672\n",
      "70 7 0.5970831513404846\n",
      "71 0 0.7044230699539185\n",
      "71 1 0.6462258100509644\n",
      "71 2 0.6334648132324219\n",
      "71 3 0.6458389759063721\n",
      "71 4 0.6026440858840942\n",
      "71 5 0.633383572101593\n",
      "71 6 0.6533396244049072\n",
      "71 7 0.6285322904586792\n",
      "72 0 0.6200694441795349\n",
      "72 1 0.6337785124778748\n",
      "72 2 0.6393982768058777\n",
      "72 3 0.6663250923156738\n",
      "72 4 0.633226752281189\n",
      "72 5 0.6465997099876404\n",
      "72 6 0.633698582649231\n",
      "72 7 0.7055598497390747\n",
      "73 0 0.6592562198638916\n",
      "73 1 0.6335111856460571\n",
      "73 2 0.6779599785804749\n",
      "73 3 0.6835124492645264\n",
      "73 4 0.6277470588684082\n",
      "73 5 0.615744411945343\n",
      "73 6 0.6335190534591675\n",
      "73 7 0.60763019323349\n",
      "74 0 0.671906054019928\n",
      "74 1 0.6149675846099854\n",
      "74 2 0.6529625654220581\n",
      "74 3 0.5887812376022339\n",
      "74 4 0.6723471879959106\n",
      "74 5 0.6596511602401733\n",
      "74 6 0.6461223363876343\n",
      "74 7 0.6496044993400574\n",
      "75 0 0.6590801477432251\n",
      "75 1 0.6523274779319763\n",
      "75 2 0.6215288639068604\n",
      "75 3 0.6464734673500061\n",
      "75 4 0.645967423915863\n",
      "75 5 0.652665913105011\n",
      "75 6 0.6338426470756531\n",
      "75 7 0.6388153433799744\n",
      "76 0 0.6144482493400574\n",
      "76 1 0.6527150869369507\n",
      "76 2 0.6658049821853638\n",
      "76 3 0.614371120929718\n",
      "76 4 0.620419442653656\n",
      "76 5 0.6590659618377686\n",
      "76 6 0.6657458543777466\n",
      "76 7 0.6724408268928528\n",
      "77 0 0.6654943823814392\n",
      "77 1 0.5957172513008118\n",
      "77 2 0.620781660079956\n",
      "77 3 0.6785390377044678\n",
      "77 4 0.6653586030006409\n",
      "77 5 0.6209638714790344\n",
      "77 6 0.6209741830825806\n",
      "77 7 0.7159376740455627\n",
      "78 0 0.6214986443519592\n",
      "78 1 0.6336914300918579\n",
      "78 2 0.5954421758651733\n",
      "78 3 0.6332964897155762\n",
      "78 4 0.7051955461502075\n",
      "78 5 0.6647608280181885\n",
      "78 6 0.6715035438537598\n",
      "78 7 0.6183695197105408\n",
      "79 0 0.6084874272346497\n",
      "79 1 0.6594268679618835\n",
      "79 2 0.6208612322807312\n",
      "79 3 0.6203500032424927\n",
      "79 4 0.6917511820793152\n",
      "79 5 0.6715814471244812\n",
      "79 6 0.634000301361084\n",
      "79 7 0.6493613719940186\n",
      "80 0 0.6404678225517273\n",
      "80 1 0.6081678867340088\n",
      "80 2 0.6522989869117737\n",
      "80 3 0.6143949031829834\n",
      "80 4 0.6725151538848877\n",
      "80 5 0.6272185444831848\n",
      "80 6 0.6848942637443542\n",
      "80 7 0.6599456071853638\n",
      "81 0 0.6211036443710327\n",
      "81 1 0.6272732615470886\n",
      "81 2 0.6591026186943054\n",
      "81 3 0.7221543788909912\n",
      "81 4 0.6400517821311951\n",
      "81 5 0.6093674302101135\n",
      "81 6 0.6277432441711426\n",
      "81 7 0.6498157978057861\n",
      "82 0 0.6464263796806335\n",
      "82 1 0.6337031722068787\n",
      "82 2 0.6271929740905762\n",
      "82 3 0.5764760375022888\n",
      "82 4 0.6985693573951721\n",
      "82 5 0.6526038646697998\n",
      "82 6 0.6906675696372986\n",
      "82 7 0.6182265281677246\n",
      "83 0 0.6721348762512207\n",
      "83 1 0.6401724219322205\n",
      "83 2 0.6083076596260071\n",
      "83 3 0.6079447865486145\n",
      "83 4 0.6334794163703918\n",
      "83 5 0.6526947021484375\n",
      "83 6 0.6783074140548706\n",
      "83 7 0.6711087822914124\n",
      "84 0 0.6463132500648499\n",
      "84 1 0.6395564675331116\n",
      "84 2 0.6400876641273499\n",
      "84 3 0.6210165619850159\n",
      "84 4 0.6591625809669495\n",
      "84 5 0.6589249968528748\n",
      "84 6 0.6712044477462769\n",
      "84 7 0.5968954563140869\n",
      "85 0 0.640247642993927\n",
      "85 1 0.6715345978736877\n",
      "85 2 0.6396914124488831\n",
      "85 3 0.6212556958198547\n",
      "85 4 0.6333702206611633\n",
      "85 5 0.6526627540588379\n",
      "85 6 0.6524201035499573\n",
      "85 7 0.6387926936149597\n",
      "86 0 0.6337927579879761\n",
      "86 1 0.6269153356552124\n",
      "86 2 0.6463496685028076\n",
      "86 3 0.6398225426673889\n",
      "86 4 0.633183479309082\n",
      "86 5 0.6912697553634644\n",
      "86 6 0.6208202838897705\n",
      "86 7 0.6716271042823792\n",
      "87 0 0.6712916493415833\n",
      "87 1 0.6710981726646423\n",
      "87 2 0.6710460782051086\n",
      "87 3 0.6281708478927612\n",
      "87 4 0.6643966436386108\n",
      "87 5 0.6216657161712646\n",
      "87 6 0.6029566526412964\n",
      "87 7 0.6068137884140015\n",
      "88 0 0.6656774878501892\n",
      "88 1 0.5759544968605042\n",
      "88 2 0.6199777722358704\n",
      "88 3 0.6462582349777222\n",
      "88 4 0.7127472162246704\n",
      "88 5 0.6458331942558289\n",
      "88 6 0.620847761631012\n",
      "88 7 0.6825318336486816\n",
      "89 0 0.6402731537818909\n",
      "89 1 0.6208217144012451\n",
      "89 2 0.633197009563446\n",
      "89 3 0.6594154834747314\n",
      "89 4 0.6204270720481873\n",
      "89 5 0.6524055004119873\n",
      "89 6 0.6781874299049377\n",
      "89 7 0.6494829654693604\n",
      "90 0 0.6393771171569824\n",
      "90 1 0.6333155035972595\n",
      "90 2 0.6077491044998169\n",
      "90 3 0.6397457718849182\n",
      "90 4 0.6720184087753296\n",
      "90 5 0.7171283960342407\n",
      "90 6 0.6337581872940063\n",
      "90 7 0.586769700050354\n",
      "91 0 0.6463680863380432\n",
      "91 1 0.6394201517105103\n",
      "91 2 0.6330665349960327\n",
      "91 3 0.646027147769928\n",
      "91 4 0.6718559265136719\n",
      "91 5 0.652324914932251\n",
      "91 6 0.6461931467056274\n",
      "91 7 0.5971183776855469\n",
      "92 0 0.6269335150718689\n",
      "92 1 0.6525622010231018\n",
      "92 2 0.6203941106796265\n",
      "92 3 0.5938287973403931\n",
      "92 4 0.6594328284263611\n",
      "92 5 0.6662460565567017\n",
      "92 6 0.6588063836097717\n",
      "92 7 0.6948097348213196\n",
      "93 0 0.6652594208717346\n",
      "93 1 0.6459371447563171\n",
      "93 2 0.6398581862449646\n",
      "93 3 0.6393353939056396\n",
      "93 4 0.6397252082824707\n",
      "93 5 0.6210581064224243\n",
      "93 6 0.6526559591293335\n",
      "93 7 0.6498041152954102\n",
      "94 0 0.6462854743003845\n",
      "94 1 0.6529513001441956\n",
      "94 2 0.6460624933242798\n",
      "94 3 0.633506178855896\n",
      "94 4 0.6518818736076355\n",
      "94 5 0.6771900653839111\n",
      "94 6 0.5714355707168579\n",
      "94 7 0.6939025521278381\n",
      "95 0 0.6647505164146423\n",
      "95 1 0.5772664546966553\n",
      "95 2 0.6203724145889282\n",
      "95 3 0.6392530202865601\n",
      "95 4 0.6983916759490967\n",
      "95 5 0.6712197065353394\n",
      "95 6 0.6403148770332336\n",
      "95 7 0.6395658254623413\n",
      "96 0 0.5832529664039612\n",
      "96 1 0.6719218492507935\n",
      "96 2 0.6268166899681091\n",
      "96 3 0.6200151443481445\n",
      "96 4 0.6397464871406555\n",
      "96 5 0.6660062670707703\n",
      "96 6 0.6650733947753906\n",
      "96 7 0.7041851878166199\n",
      "97 0 0.5963200330734253\n",
      "97 1 0.6267244815826416\n",
      "97 2 0.6716631054878235\n",
      "97 3 0.6209374070167542\n",
      "97 4 0.6652684211730957\n",
      "97 5 0.6648219227790833\n",
      "97 6 0.6211617588996887\n",
      "97 7 0.7143622040748596\n",
      "98 0 0.6707836985588074\n",
      "98 1 0.6400489211082458\n",
      "98 2 0.7006545066833496\n",
      "98 3 0.5864027142524719\n",
      "98 4 0.6894120573997498\n",
      "98 5 0.5865465402603149\n",
      "98 6 0.6521559953689575\n",
      "98 7 0.6171713471412659\n",
      "99 0 0.627560019493103\n",
      "99 1 0.6395871043205261\n",
      "99 2 0.7283352613449097\n",
      "99 3 0.6097959280014038\n",
      "99 4 0.6208363175392151\n",
      "99 5 0.6521881222724915\n",
      "99 6 0.5958964228630066\n",
      "99 7 0.7039780616760254\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.l1 = torch.nn.Linear(8, 6)\n",
    "        self.l2 = torch.nn.Linear(6, 4)\n",
    "        self.l3 = torch.nn.Linear(4, 1)\n",
    "\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        out1 = self.sigmoid(self.l1(x))\n",
    "        out2 = self.sigmoid(self.l2(out1))\n",
    "        y_pred = self.sigmoid(self.l3(out2))\n",
    "        return y_pred\n",
    "\n",
    "# our model\n",
    "model = Model()\n",
    "criterion = torch.nn.BCELoss(size_average=True )\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(100):\n",
    "    for i, data in enumerate(train_loader):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Forward pass\n",
    "        y_pred = model(inputs)\n",
    "\n",
    "        # Compute and print loss\n",
    "        loss = criterion(y_pred, labels)\n",
    "        print(epoch, i, loss.data.item())\n",
    "\n",
    "        # Zero gradients, perform a backward pass, and update the weights.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Control Flow + Weight sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example of dynamic graphs and weight sharing, we implement a very strange model: a fully-connected ReLU network that on each forward pass chooses a random number between 1 and 4 and uses that many hidden layers, reusing the same weights multiple times to compute the innermost hidden layers.\n",
    "For this model we can use normal Python flow control to implement the loop, and we can implement weight sharing among the innermost layers by simply reusing the same Module multiple times when defining the forward pass.\n",
    "We can easily implement this model as a Module subclass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 677.3978881835938\n",
      "1 672.4699096679688\n",
      "2 700.7490234375\n",
      "3 670.8538818359375\n",
      "4 606.662109375\n",
      "5 666.2186279296875\n",
      "6 640.1378173828125\n",
      "7 659.9547119140625\n",
      "8 621.1004638671875\n",
      "9 664.2947387695312\n",
      "10 359.8459167480469\n",
      "11 661.3030395507812\n",
      "12 575.29345703125\n",
      "13 277.2897033691406\n",
      "14 244.0753631591797\n",
      "15 203.68663024902344\n",
      "16 161.58023071289062\n",
      "17 648.961669921875\n",
      "18 98.31787109375\n",
      "19 481.28948974609375\n",
      "20 457.67791748046875\n",
      "21 421.7212219238281\n",
      "22 375.3182373046875\n",
      "23 564.3938598632812\n",
      "24 140.8961639404297\n",
      "25 240.03411865234375\n",
      "26 150.39370727539062\n",
      "27 551.97802734375\n",
      "28 122.7464599609375\n",
      "29 96.68204498291016\n",
      "30 475.8066711425781\n",
      "31 144.64871215820312\n",
      "32 129.384521484375\n",
      "33 403.78546142578125\n",
      "34 364.97369384765625\n",
      "35 260.8961486816406\n",
      "36 56.2715950012207\n",
      "37 51.43171691894531\n",
      "38 234.08982849121094\n",
      "39 42.36616897583008\n",
      "40 42.942440032958984\n",
      "41 175.8275604248047\n",
      "42 20.457427978515625\n",
      "43 179.55850219726562\n",
      "44 161.96942138671875\n",
      "45 99.48065185546875\n",
      "46 53.00577926635742\n",
      "47 410.46136474609375\n",
      "48 231.00315856933594\n",
      "49 115.35193634033203\n",
      "50 277.9552307128906\n",
      "51 215.86700439453125\n",
      "52 220.59951782226562\n",
      "53 231.52401733398438\n",
      "54 226.20118713378906\n",
      "55 199.93475341796875\n",
      "56 114.86759185791016\n",
      "57 77.9510498046875\n",
      "58 158.72256469726562\n",
      "59 57.76521682739258\n",
      "60 140.50267028808594\n",
      "61 102.20631408691406\n",
      "62 34.523406982421875\n",
      "63 143.7626953125\n",
      "64 55.5262565612793\n",
      "65 62.51278305053711\n",
      "66 51.10929489135742\n",
      "67 45.3814697265625\n",
      "68 85.32862854003906\n",
      "69 28.472225189208984\n",
      "70 35.980960845947266\n",
      "71 42.52322006225586\n",
      "72 122.80140686035156\n",
      "73 56.74860382080078\n",
      "74 81.75159454345703\n",
      "75 53.34793472290039\n",
      "76 34.94329833984375\n",
      "77 75.88146209716797\n",
      "78 36.896583557128906\n",
      "79 22.999061584472656\n",
      "80 24.562625885009766\n",
      "81 26.971263885498047\n",
      "82 19.473583221435547\n",
      "83 54.15983581542969\n",
      "84 15.696796417236328\n",
      "85 24.482545852661133\n",
      "86 28.80060386657715\n",
      "87 22.36098861694336\n",
      "88 28.23798179626465\n",
      "89 11.573202133178711\n",
      "90 29.398666381835938\n",
      "91 30.49960708618164\n",
      "92 23.30950355529785\n",
      "93 22.872055053710938\n",
      "94 7.688765048980713\n",
      "95 6.341662883758545\n",
      "96 6.634223937988281\n",
      "97 22.438329696655273\n",
      "98 19.266679763793945\n",
      "99 25.102527618408203\n",
      "100 26.154205322265625\n",
      "101 26.124317169189453\n",
      "102 13.504249572753906\n",
      "103 11.302956581115723\n",
      "104 13.755997657775879\n",
      "105 18.39789390563965\n",
      "106 12.329715728759766\n",
      "107 26.493606567382812\n",
      "108 11.558364868164062\n",
      "109 16.32860565185547\n",
      "110 14.527457237243652\n",
      "111 8.71272087097168\n",
      "112 6.805548667907715\n",
      "113 5.962965965270996\n",
      "114 57.34401321411133\n",
      "115 5.076995372772217\n",
      "116 15.769124031066895\n",
      "117 24.06696319580078\n",
      "118 11.154603004455566\n",
      "119 16.265539169311523\n",
      "120 8.030213356018066\n",
      "121 13.14787769317627\n",
      "122 13.546828269958496\n",
      "123 36.03303909301758\n",
      "124 7.370810031890869\n",
      "125 4.762807369232178\n",
      "126 34.1837158203125\n",
      "127 9.663801193237305\n",
      "128 6.552972316741943\n",
      "129 7.724306106567383\n",
      "130 10.062373161315918\n",
      "131 16.817686080932617\n",
      "132 5.074821472167969\n",
      "133 4.15778923034668\n",
      "134 7.532009124755859\n",
      "135 7.203123569488525\n",
      "136 6.119343280792236\n",
      "137 15.7695894241333\n",
      "138 15.622793197631836\n",
      "139 1.7023169994354248\n",
      "140 12.144207000732422\n",
      "141 8.842041015625\n",
      "142 3.052932024002075\n",
      "143 5.242491245269775\n",
      "144 14.477462768554688\n",
      "145 3.011089324951172\n",
      "146 7.244319915771484\n",
      "147 7.772792816162109\n",
      "148 5.050580978393555\n",
      "149 4.358532428741455\n",
      "150 4.73559045791626\n",
      "151 4.534285545349121\n",
      "152 3.1395280361175537\n",
      "153 2.2262861728668213\n",
      "154 46.6683349609375\n",
      "155 3.6882340908050537\n",
      "156 7.2510576248168945\n",
      "157 17.228487014770508\n",
      "158 30.508264541625977\n",
      "159 2.0494959354400635\n",
      "160 10.284703254699707\n",
      "161 11.036511421203613\n",
      "162 10.150064468383789\n",
      "163 11.448479652404785\n",
      "164 3.110327959060669\n",
      "165 9.460101127624512\n",
      "166 4.945799827575684\n",
      "167 4.811369895935059\n",
      "168 3.5619428157806396\n",
      "169 2.085577964782715\n",
      "170 3.7045443058013916\n",
      "171 4.0933051109313965\n",
      "172 3.021596670150757\n",
      "173 4.987110614776611\n",
      "174 2.6584677696228027\n",
      "175 3.4138505458831787\n",
      "176 4.914666652679443\n",
      "177 1.5806225538253784\n",
      "178 1.0166544914245605\n",
      "179 7.734450817108154\n",
      "180 1.6874467134475708\n",
      "181 2.4576213359832764\n",
      "182 2.9706032276153564\n",
      "183 2.12178635597229\n",
      "184 4.972002029418945\n",
      "185 1.6887093782424927\n",
      "186 1.012904167175293\n",
      "187 5.257813453674316\n",
      "188 2.824636697769165\n",
      "189 4.075179576873779\n",
      "190 2.255648374557495\n",
      "191 3.869361639022827\n",
      "192 0.8540657162666321\n",
      "193 1.4077690839767456\n",
      "194 2.4388811588287354\n",
      "195 2.3989598751068115\n",
      "196 1.5777430534362793\n",
      "197 1.3993481397628784\n",
      "198 1.2743184566497803\n",
      "199 3.463439464569092\n",
      "200 0.9532532691955566\n",
      "201 0.8699803352355957\n",
      "202 1.403496503829956\n",
      "203 3.8792977333068848\n",
      "204 1.149449110031128\n",
      "205 1.2699968814849854\n",
      "206 3.0117814540863037\n",
      "207 0.7549842596054077\n",
      "208 0.7193689346313477\n",
      "209 2.48822021484375\n",
      "210 1.1969541311264038\n",
      "211 1.117806315422058\n",
      "212 0.45811396837234497\n",
      "213 2.339930534362793\n",
      "214 1.8329962491989136\n",
      "215 1.1447982788085938\n",
      "216 2.3271398544311523\n",
      "217 1.5729178190231323\n",
      "218 1.133301854133606\n",
      "219 1.4594017267227173\n",
      "220 1.5230295658111572\n",
      "221 0.5982105731964111\n",
      "222 1.5915108919143677\n",
      "223 1.103796124458313\n",
      "224 1.494208812713623\n",
      "225 1.5395628213882446\n",
      "226 1.8036385774612427\n",
      "227 1.1394833326339722\n",
      "228 0.9485232830047607\n",
      "229 0.8578622341156006\n",
      "230 1.0678707361221313\n",
      "231 0.6094550490379333\n",
      "232 8.689422607421875\n",
      "233 0.6962307691574097\n",
      "234 3.093101978302002\n",
      "235 7.901375770568848\n",
      "236 0.5664381980895996\n",
      "237 0.5831100940704346\n",
      "238 0.9632905721664429\n",
      "239 4.9830322265625\n",
      "240 11.719271659851074\n",
      "241 2.8086118698120117\n",
      "242 2.9817843437194824\n",
      "243 9.446621894836426\n",
      "244 6.765115261077881\n",
      "245 2.915980339050293\n",
      "246 2.5740842819213867\n",
      "247 0.9421494603157043\n",
      "248 25.955408096313477\n",
      "249 0.34907764196395874\n",
      "250 1.6820149421691895\n",
      "251 19.574968338012695\n",
      "252 7.002603054046631\n",
      "253 5.661647796630859\n",
      "254 13.735973358154297\n",
      "255 5.108273029327393\n",
      "256 4.203219890594482\n",
      "257 23.579999923706055\n",
      "258 2.1607043743133545\n",
      "259 6.466104507446289\n",
      "260 3.7582149505615234\n",
      "261 6.152734756469727\n",
      "262 1.5875383615493774\n",
      "263 11.073357582092285\n",
      "264 2.917941093444824\n",
      "265 5.140137195587158\n",
      "266 8.88076400756836\n",
      "267 3.2999439239501953\n",
      "268 1.572585105895996\n",
      "269 2.6026246547698975\n",
      "270 3.394852638244629\n",
      "271 43.13433074951172\n",
      "272 4.00590705871582\n",
      "273 3.999917984008789\n",
      "274 10.673002243041992\n",
      "275 8.624533653259277\n",
      "276 2.464036464691162\n",
      "277 3.0803346633911133\n",
      "278 11.404273986816406\n",
      "279 3.1282262802124023\n",
      "280 1.6641510725021362\n",
      "281 3.0054612159729004\n",
      "282 8.091927528381348\n",
      "283 3.337110757827759\n",
      "284 2.8726441860198975\n",
      "285 3.136099100112915\n",
      "286 2.438361644744873\n",
      "287 12.359282493591309\n",
      "288 2.769704818725586\n",
      "289 2.2321295738220215\n",
      "290 1.9187580347061157\n",
      "291 6.603264331817627\n",
      "292 3.0415477752685547\n",
      "293 2.728590726852417\n",
      "294 1.4918076992034912\n",
      "295 0.9079999327659607\n",
      "296 4.670773506164551\n",
      "297 0.7330743670463562\n",
      "298 0.9518358707427979\n",
      "299 0.9960119128227234\n",
      "300 2.097933530807495\n",
      "301 0.6173173785209656\n",
      "302 0.7555601596832275\n",
      "303 0.7495344281196594\n",
      "304 4.635914325714111\n",
      "305 1.2148605585098267\n",
      "306 0.4099474847316742\n",
      "307 1.5288429260253906\n",
      "308 6.423258304595947\n",
      "309 2.3557910919189453\n",
      "310 1.3838000297546387\n",
      "311 4.224212169647217\n",
      "312 4.14573335647583\n",
      "313 1.3843015432357788\n",
      "314 1.0687565803527832\n",
      "315 2.521538496017456\n",
      "316 1.8026748895645142\n",
      "317 1.1751569509506226\n",
      "318 1.5701664686203003\n",
      "319 1.2872602939605713\n",
      "320 0.4885989725589752\n",
      "321 3.6759440898895264\n",
      "322 1.9327911138534546\n",
      "323 1.174154281616211\n",
      "324 1.4831743240356445\n",
      "325 1.3457585573196411\n",
      "326 8.265460014343262\n",
      "327 1.249348759651184\n",
      "328 0.6214140057563782\n",
      "329 1.8510652780532837\n",
      "330 6.633152484893799\n",
      "331 4.011318206787109\n",
      "332 0.7009398341178894\n",
      "333 0.24951237440109253\n",
      "334 5.060296058654785\n",
      "335 6.236688137054443\n",
      "336 0.3312552869319916\n",
      "337 8.646096229553223\n",
      "338 0.4712753891944885\n",
      "339 2.3235297203063965\n",
      "340 5.026124000549316\n",
      "341 7.113504886627197\n",
      "342 3.5080010890960693\n",
      "343 1.9846768379211426\n",
      "344 2.680206298828125\n",
      "345 3.4636213779449463\n",
      "346 6.45277738571167\n",
      "347 1.7730621099472046\n",
      "348 1.9437474012374878\n",
      "349 1.0032886266708374\n",
      "350 1.4055907726287842\n",
      "351 3.380680799484253\n",
      "352 0.9599280953407288\n",
      "353 1.8742029666900635\n",
      "354 2.1041367053985596\n",
      "355 1.8854714632034302\n",
      "356 1.4987661838531494\n",
      "357 2.885582685470581\n",
      "358 3.264063596725464\n",
      "359 4.253734111785889\n",
      "360 1.6864311695098877\n",
      "361 0.9911766052246094\n",
      "362 7.542063236236572\n",
      "363 2.3839523792266846\n",
      "364 2.456947088241577\n",
      "365 1.478578805923462\n",
      "366 1.484501600265503\n",
      "367 6.716694355010986\n",
      "368 0.3426109850406647\n",
      "369 5.3895583152771\n",
      "370 1.6723120212554932\n",
      "371 3.895287275314331\n",
      "372 5.02310848236084\n",
      "373 17.973175048828125\n",
      "374 2.017280340194702\n",
      "375 4.55042028427124\n",
      "376 2.770322799682617\n",
      "377 15.024077415466309\n",
      "378 56.23322677612305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "379 1.4783087968826294\n",
      "380 18.314647674560547\n",
      "381 51.904884338378906\n",
      "382 18.268234252929688\n",
      "383 4.802370071411133\n",
      "384 10.454974174499512\n",
      "385 0.906532347202301\n",
      "386 15.953115463256836\n",
      "387 1.572950005531311\n",
      "388 1.129738450050354\n",
      "389 46.75904083251953\n",
      "390 2.0352675914764404\n",
      "391 5.6702165603637695\n",
      "392 4.322254180908203\n",
      "393 4.696719169616699\n",
      "394 13.231590270996094\n",
      "395 7.289726257324219\n",
      "396 5.148725509643555\n",
      "397 3.425673246383667\n",
      "398 4.7888336181640625\n",
      "399 7.813744068145752\n",
      "400 2.041982650756836\n",
      "401 1.7228134870529175\n",
      "402 3.482633590698242\n",
      "403 2.5991673469543457\n",
      "404 3.2600789070129395\n",
      "405 1.8871153593063354\n",
      "406 1.9047726392745972\n",
      "407 1.6865788698196411\n",
      "408 2.2055680751800537\n",
      "409 1.9000658988952637\n",
      "410 2.945162296295166\n",
      "411 1.4536107778549194\n",
      "412 1.3652088642120361\n",
      "413 2.598534107208252\n",
      "414 0.8209268450737\n",
      "415 8.582776069641113\n",
      "416 5.291280746459961\n",
      "417 10.996499061584473\n",
      "418 4.261285781860352\n",
      "419 1.1843098402023315\n",
      "420 0.9513518810272217\n",
      "421 2.149409294128418\n",
      "422 2.5198466777801514\n",
      "423 1.0805236101150513\n",
      "424 1.8632349967956543\n",
      "425 1.4446494579315186\n",
      "426 1.2346795797348022\n",
      "427 0.8827336430549622\n",
      "428 3.7744340896606445\n",
      "429 2.0868453979492188\n",
      "430 0.5849024653434753\n",
      "431 0.5576733946800232\n",
      "432 2.717728614807129\n",
      "433 9.196531295776367\n",
      "434 1.0277360677719116\n",
      "435 3.2791013717651367\n",
      "436 6.324779033660889\n",
      "437 3.8503165245056152\n",
      "438 3.1303176879882812\n",
      "439 0.9341526031494141\n",
      "440 1.0215510129928589\n",
      "441 1.8554567098617554\n",
      "442 2.5624806880950928\n",
      "443 2.3874897956848145\n",
      "444 3.1114213466644287\n",
      "445 9.312957763671875\n",
      "446 0.5816623568534851\n",
      "447 1.9432681798934937\n",
      "448 5.474803447723389\n",
      "449 11.942731857299805\n",
      "450 2.000734567642212\n",
      "451 0.6623708009719849\n",
      "452 0.6028662323951721\n",
      "453 3.177398204803467\n",
      "454 2.627671480178833\n",
      "455 1.2587817907333374\n",
      "456 1.3464338779449463\n",
      "457 2.861342668533325\n",
      "458 0.5106127262115479\n",
      "459 0.5619597434997559\n",
      "460 0.955315113067627\n",
      "461 1.0525869131088257\n",
      "462 8.164599418640137\n",
      "463 5.552643775939941\n",
      "464 1.057322382926941\n",
      "465 2.209571123123169\n",
      "466 9.880041122436523\n",
      "467 3.0836827754974365\n",
      "468 0.7417157888412476\n",
      "469 0.9740735292434692\n",
      "470 4.6153669357299805\n",
      "471 1.435749888420105\n",
      "472 3.9422261714935303\n",
      "473 0.9974976778030396\n",
      "474 0.45795103907585144\n",
      "475 1.2500770092010498\n",
      "476 2.2893097400665283\n",
      "477 5.708540439605713\n",
      "478 1.6703591346740723\n",
      "479 1.57108736038208\n",
      "480 0.26139259338378906\n",
      "481 8.374046325683594\n",
      "482 0.3117567300796509\n",
      "483 2.8815386295318604\n",
      "484 0.9897573590278625\n",
      "485 0.6096639633178711\n",
      "486 3.379586935043335\n",
      "487 1.9938938617706299\n",
      "488 11.388874053955078\n",
      "489 2.6016685962677\n",
      "490 2.084336757659912\n",
      "491 7.913020133972168\n",
      "492 6.677400588989258\n",
      "493 4.586461067199707\n",
      "494 1.0581344366073608\n",
      "495 0.4405471086502075\n",
      "496 11.465452194213867\n",
      "497 1.925547480583191\n",
      "498 4.02564001083374\n",
      "499 0.252044141292572\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import random\n",
    "import torch\n",
    "\n",
    "\n",
    "class DynamicNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        In the constructor we construct three nn.Linear instances that we will use\n",
    "        in the forward pass.\n",
    "        \"\"\"\n",
    "        super(DynamicNet, self).__init__()\n",
    "        self.input_linear = torch.nn.Linear(D_in, H)\n",
    "        self.middle_linear = torch.nn.Linear(H, H)\n",
    "        self.output_linear = torch.nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        For the forward pass of the model, we randomly choose either 0, 1, 2, or 3\n",
    "        and reuse the middle_linear Module that many times to compute hidden layer\n",
    "        representations.\n",
    "\n",
    "        Since each forward pass builds a dynamic computation graph, we can use normal\n",
    "        Python control-flow operators like loops or conditional statements when\n",
    "        defining the forward pass of the model.\n",
    "\n",
    "        Here we also see that it is perfectly safe to reuse the same Module many\n",
    "        times when defining a computational graph. This is a big improvement from Lua\n",
    "        Torch, where each Module could be used only once.\n",
    "        \"\"\"\n",
    "        h_relu = self.input_linear(x).clamp(min=0)\n",
    "        for _ in range(random.randint(0, 3)):\n",
    "            h_relu = self.middle_linear(h_relu).clamp(min=0)\n",
    "        y_pred = self.output_linear(h_relu)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# Construct our model by instantiating the class defined above\n",
    "model = DynamicNet(D_in, H, D_out)\n",
    "\n",
    "# Construct our loss function and an Optimizer. Training this strange model with\n",
    "# vanilla stochastic gradient descent is tough, so we use momentum\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)\n",
    "for t in range(500):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
